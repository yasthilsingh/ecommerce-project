{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 Problem Statement\n",
    "\n",
    "Which customer segments drive the most revenue and how can we target them effectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E‑commerce RFM & CLV Analysis 📊\n",
    "\n",
    "**Objective**  \n",
    "Identify high‑value customer segments and quantify revenue potential using RFM scoring and 12‑month Customer Lifetime Value (CLV) estimates.  \n",
    "Outputs include actionable insights for retention, upselling, and acquisition.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Roadmap  \n",
    "1. **Setup & Imports**  \n",
    "2. **Data Loading & Cleaning**  \n",
    "3. **RFM Feature Engineering**  \n",
    "4. **CLV Modeling**  \n",
    "5. **Segmentation & Visualization**  \n",
    "6. **Insights & Business Recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyXPgr07PP8K"
   },
   "source": [
    "# Project Description\n",
    "\n",
    "**Which customer segments drive the most revenue and lifetime value, and how can we identify and target these high-value cohorts effectively?**\n",
    "\n",
    "Understanding which customer segments drive the most revenue and lifetime value is an indicator of which products are serving the needs of the customer the best.\n",
    "\n",
    "- How many times a customer will make a purchase in the future (frequency prediction)\n",
    "\n",
    "- How recently they've purchased (recency)\n",
    "\n",
    "- How long you've observed them (T, the age of the customer)\n",
    "\n",
    "- Segmenting customers into personas to find the most profitable users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPDJfn_KSbOo"
   },
   "source": [
    "# Data Cleaning and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:46.361542Z",
     "start_time": "2025-05-30T20:12:44.931062Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuaKbQu73GWn",
    "outputId": "fe8b609f-fb90-4df7-9f50-083d6b3c9eee",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install plotly\n",
    "!pip install Lifetimes\n",
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:46.754389Z",
     "start_time": "2025-05-30T20:12:46.365641Z"
    },
    "id": "v4js_86w1xXD",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "ecom_path = kagglehub.dataset_download('carrie1/ecommerce-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:46.765123Z",
     "start_time": "2025-05-30T20:12:46.762961Z"
    },
    "id": "2OjiuzdR2XvX",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🗂 Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:47.095846Z",
     "start_time": "2025-05-30T20:12:46.769961Z"
    },
    "id": "KeTDbIIW2rE4",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(ecom_path, 'data.csv'), encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:47.111777Z",
     "start_time": "2025-05-30T20:12:47.103689Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "uc9R5TlC3Fat",
    "outputId": "c263e0ba-9c96-415b-a882-032026307970",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:47.120239Z",
     "start_time": "2025-05-30T20:12:47.117939Z"
    },
    "id": "i_Rsis4rbwXe",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "def Df_info(df):\n",
    "    print(\"########### Head ###########\")\n",
    "    print(df.head())\n",
    "    print(\"########### Tail ###########\")\n",
    "    print(df.tail())\n",
    "    print(\"########### Shape ###########\")\n",
    "    print(df.shape)\n",
    "    print(\"########### Info ###########\")\n",
    "    print(df.info())\n",
    "    print(\"########### Info ###########\")\n",
    "    print(df.columns)\n",
    "    print(\"########### Quantiles ###########\")\n",
    "    print(df.describe().T)\n",
    "    print(\"########### NA ###########\")\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:47.262877Z",
     "start_time": "2025-05-30T20:12:47.136728Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIW0JuMHcAs2",
    "outputId": "f5796956-c4e0-415b-87dd-2a424cd2e005",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "print(Df_info(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1WVK0CNEMAz"
   },
   "source": [
    "**Column Descritpions:**\n",
    "\n",
    "\n",
    "\n",
    "InvoiceNo: Invoice number that consists 6 digits. If this code starts with letter 'c', it indicates a cancellation.\n",
    "\n",
    "StockCode: Product code that consists 5 digits.\n",
    "\n",
    "Description: Product name.\n",
    "\n",
    "Quantity: The quantities of each product per transaction.\n",
    "\n",
    "InvoiceDate: Represents the day and time when each transaction was generated.\n",
    "\n",
    "UnitPrice: Product price per unit in sterling (Â£).\n",
    "\n",
    "CustomerID: Customer number that consists 5 digits. Each customer has a unique customer ID.\n",
    "\n",
    "Country: Name of the country where each customer resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:47.457091Z",
     "start_time": "2025-05-30T20:12:47.324033Z"
    },
    "id": "qq_i9KcjSspx",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# ensuring column names are presented in a 'pythonic' way:\n",
    "\n",
    "df.rename(index=str, columns={'InvoiceNo': 'invoice_no',\n",
    "                              'StockCode' : 'stock_code',\n",
    "                              'Description' : 'description',\n",
    "                              'Quantity' : 'quantity',\n",
    "                              'InvoiceDate' : 'invoice_date',\n",
    "                              'UnitPrice' : 'unit_price',\n",
    "                              'CustomerID' : 'customer_id',\n",
    "                              'Country' : 'country'}, inplace=True)\n",
    "\n",
    "# Drop Columns that have missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Complete Rows which are complete duplicates:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧹 Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:47.669202Z",
     "start_time": "2025-05-30T20:12:47.464469Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "def find_duplicates(df):\n",
    "\n",
    "    duplicates = df[df.duplicated(keep= False)]\n",
    "\n",
    "    df_clean = df.drop_duplicates()\n",
    "\n",
    "    print(f'Dropped {len(duplicates)} duplicate rows')\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "df = find_duplicates(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:47.746653Z",
     "start_time": "2025-05-30T20:12:47.690246Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "def reorder_columns(df, columns):\n",
    "    return df[columns]\n",
    "\n",
    "cols = ['customer_id', 'invoice_no', 'stock_code', 'quantity', 'description', 'invoice_date', 'unit_price', 'country']\n",
    "df = reorder_columns(df, cols)\n",
    "df.head()\n",
    "\n",
    "def to_date_time(df, column):\n",
    "\n",
    "    df[column] = pd.to_datetime(df[column])\n",
    "\n",
    "    return df\n",
    "\n",
    "df = to_date_time(df, 'invoice_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo5vRqsfdBmH"
   },
   "source": [
    "## Visualising Descriptive Statistics and Interquartile Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:47.867200Z",
     "start_time": "2025-05-30T20:12:47.763434Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "bnNZ4QpWd4Yz",
    "outputId": "5d08ed61-dff0-43d4-a514-edb507906ecc",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "def find_out(dataframe):\n",
    "\n",
    "# finding the inter quartile range (IQR) - focus on the middle 50% of values\n",
    "\n",
    "  desc = dataframe.describe().T\n",
    "\n",
    "  # Calculate IQR\n",
    "  desc['IQR'] = desc['75%'] - desc['25%']\n",
    "\n",
    "  # Add null count and unique count\n",
    "  desc['nulls'] = dataframe.isnull().sum()\n",
    "  desc['n_unique'] = dataframe.nunique()\n",
    "\n",
    "  # are there outliers - used a threshold of +/- (1.5 * IQR)\n",
    "\n",
    "  desc['max_outlier'] = desc['max'] > (desc['75%'] + 1.5 * desc['IQR'])\n",
    "  desc['min_outlier'] = desc['min'] < (desc['25%'] - 1.5 * desc['IQR'])\n",
    "\n",
    "\n",
    "  #Select columns to display as heatmap - derived from the dataframe.decribe() function\n",
    "  out_df = desc[['mean', 'std', 'min', '25%', '50%', '75%', 'max', 'IQR', 'nulls', 'n_unique']]\n",
    "\n",
    "  # Plot heatmap\n",
    "  f, ax = plt.subplots(figsize=(12, out_df.shape[0] * 0.75))\n",
    "  sns.heatmap(out_df,\n",
    "              annot=True,\n",
    "              cmap=\"Blues\",\n",
    "              fmt=\".2f\",\n",
    "              ax=ax,\n",
    "              linecolor=\"white\",\n",
    "              linewidths=1.1,\n",
    "              cbar=False,\n",
    "              annot_kws={\"size\": 10})\n",
    "\n",
    "  plt.xticks(size=12)\n",
    "  plt.yticks(size=12, rotation=0)\n",
    "  plt.title(\"Descriptive Statistics & Outlier Detection\", size=14)\n",
    "  plt.show()\n",
    "\n",
    "# only include the numerical datatypes\n",
    "\n",
    "find_out(df.select_dtypes(include = [float, int]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kvy_plU8sQh_"
   },
   "source": [
    "## Inferences:\n",
    "\n",
    "Quantity:\n",
    "\n",
    "1. Average Quantity per transaction is around 12 units.\n",
    "2. std is relatively large at around 250 units per transaction.\n",
    "3.**Quantity min is showing negative values meaning that a negative quanity of products were purchased - this could indicate a return from the customer or potential canceled orders**\n",
    "4. IQR is 10 units\n",
    "5. **max value is unrealistically higher than the IQR - this is abnormally large**\n",
    "\n",
    "Unit Price:\n",
    "\n",
    "1. Average unit price was 3.46\n",
    "2. std at around 70 GBP\n",
    "3. **Min is 0** this indicates unrealistic outliers- this could indicate promotional items such as giveaways.\n",
    "4. **Should we eliminate the large max value ?** this could be helpful to our analysis ?\n",
    "5. There are 620 unique unit prices - wholesale dataset  \n",
    "\n",
    "Customer ID:\n",
    "1. There are 4372 unique customers\n",
    "\n",
    "Considerations:\n",
    "1. Quantity - Cap Values\n",
    "2. Unit - Cap Values\n",
    "3. **Investigate the Order numbers marked with C to describe canceled orders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:47.998839Z",
     "start_time": "2025-05-30T20:12:47.890718Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "def remove_canceled_orders(df):\n",
    "    \"\"\"\n",
    "    Removes canceled transactions from the dataset.\n",
    "    Canceled transactions are typically indicated by invoice_no starting with 'C'.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input transaction-level data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned data without canceled transactions.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['invoice_no'] = df['invoice_no'].astype(str)\n",
    "    df = df[~df['invoice_no'].str.startswith('C')]\n",
    "    return df\n",
    "# Clean your original dataset\n",
    "df = remove_canceled_orders(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:15:43.152910Z",
     "start_time": "2025-05-30T20:15:43.132295Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqMSp21Higyt",
    "outputId": "357a9ca3-ea04-4b7f-be32-ba11985e41c9",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# find the total number of distinct invoice numbers vs the total invoice numbers\n",
    "\n",
    "# total rows (all „invoice_no“ values)\n",
    "total_invoices = len(df['invoice_no'])\n",
    "\n",
    "# how many distinct invoices\n",
    "unique_invoices = df['invoice_no'].nunique()\n",
    "\n",
    "print(f\"Total rows: {total_invoices:,}\")      # e.g. 406,529\n",
    "print(f\"Distinct invoices: {unique_invoices:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Build Customer-Level Summary Table\n",
    "\n",
    "This function transforms raw invoice-level transaction data into **customer-level aggregates** for use in RFM scoring and CLV modeling.\n",
    "\n",
    "**What it does:**\n",
    "- Converts the `invoice_date` column to datetime format for accurate date math.\n",
    "- Calculates `line_total` for each row as `unit_price × quantity`.\n",
    "- Identifies the most recent purchase date in the dataset to compute **recency**.\n",
    "- Groups by `customer_id` to compute the following features:\n",
    "  - 🧾 `orders`: number of unique invoices (proxy for purchase frequency).\n",
    "  - 🗓️ `first_date`, `last_date`: customer’s first and most recent purchase dates.\n",
    "  - 💰 `monetary`: total spend across all orders.\n",
    "  - 📊 `avg_order_value`: average basket size per transaction.\n",
    "  - 🌍 `country`: preserved as-is (first observed country per customer).\n",
    "- Derives:\n",
    "  - 📅 `recency`: days since the last purchase.\n",
    "  - 🔁 `frequency`: number of distinct orders.\n",
    "  - ⏳ `obs_months`: time span (in months) between first and last purchase, clipped at minimum of 1 to avoid division issues.\n",
    "\n",
    "**Why it's important:**\n",
    "This table condenses raw transactional logs into a **compact customer view** that feeds directly into:\n",
    "- 📐 RFM scoring logic\n",
    "- 📈 CLV predictive models\n",
    "- 📦 Customer segmentation\n",
    "\n",
    "**Returns:** A DataFrame with one row per customer and engineered fields that describe their purchase behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔁 Feature Engineering: RFM Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:48.232473Z",
     "start_time": "2025-05-30T20:12:48.230097Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "def build_customer_summary(df):\n",
    "    \"\"\"\n",
    "    Aggregates invoice-level data to generate customer-level metrics for RFM and CLV analysis.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Must include columns ['invoice_no', 'invoice_date', 'customer_id',\n",
    "                                                 'unit_price', 'quantity', 'country']\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated customer-level DataFrame with:\n",
    "                      ['orders', 'first_date', 'last_date', 'monetary', 'avg_order_value',\n",
    "                       'country', 'recency', 'frequency', 'obs_months']\n",
    "    \"\"\"\n",
    "    # Ensure datetime format for date column\n",
    "    df['order_date'] = pd.to_datetime(df['invoice_date'])\n",
    "\n",
    "    # Precompute line total = unit_price × quantity\n",
    "    df['line_total'] = df['unit_price'] * df['quantity']\n",
    "\n",
    "    # Track latest date for recency calc\n",
    "    max_order_date = df['order_date'].max()\n",
    "\n",
    "    # Group by customer and aggregate\n",
    "    cust = df.groupby('customer_id').agg(\n",
    "        orders=('invoice_no', 'nunique'),\n",
    "        first_date=('order_date', 'min'),\n",
    "        last_date=('order_date', 'max'),\n",
    "        monetary=('line_total', 'sum'),\n",
    "        avg_order_value=('line_total', 'mean'),\n",
    "        country=('country', 'first')\n",
    "    )\n",
    "\n",
    "    # Add recency, frequency, and observation months\n",
    "    cust = cust.assign(\n",
    "        recency=(max_order_date - cust['last_date']).dt.days,\n",
    "        frequency=cust['orders'],\n",
    "        obs_months=((cust['last_date'] - cust['first_date']).dt.days / 30).clip(lower=1)\n",
    "    )\n",
    "\n",
    "    return cust.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:48.462072Z",
     "start_time": "2025-05-30T20:12:48.408812Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# 1. Build customer table\n",
    "cust = build_customer_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧮 Compute RFM Scores and Estimate 12-Month CLV\n",
    "\n",
    "This function takes a **customer-level summary DataFrame** and enhances it with:\n",
    "1. **RFM Scores** (Recency, Frequency, Monetary)\n",
    "2. A **12-month CLV estimate** using a simplified revenue projection formula.\n",
    "\n",
    "---\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "#### 🔢 RFM Scoring\n",
    "- **Recency Score**: Lower recency (i.e., more recent activity) is better. Customers are ranked into 5 quantiles (`pd.qcut` on `-recency`).\n",
    "- **Frequency Score**: Higher number of orders = higher score.\n",
    "- **Monetary Score**: Total amount spent across all purchases.\n",
    "\n",
    "Each score ranges from **1 (lowest)** to **5 (highest)**.\n",
    "\n",
    "- ✅ Adds a new column `rfm_total` = sum of all three scores.\n",
    "- 🎯 Used to rank customers and segment them (e.g., into “Champions”, “At-Risk”, etc.)\n",
    "\n",
    "#### 💸 CLV Estimate\n",
    "Calculates **12-month Customer Lifetime Value (CLV)** using a basic revenue projection formula\n",
    "This gives an expected value from each customer over the next year based on their past behavior.\n",
    "\n",
    "---\n",
    "\n",
    "**Why it's important:**\n",
    "- 🎯 **RFM scores** help with segmentation, targeting, and prioritizing customer retention efforts.\n",
    "- 📈 **CLV** predicts future revenue potential, useful for ROI-driven marketing and strategic planning.\n",
    "\n",
    "**Returns:** The original customer-level DataFrame with five new columns:\n",
    "- `recency_score`\n",
    "- `frequency_score`\n",
    "- `monetary_score`\n",
    "- `rfm_total`\n",
    "- `clv_12mo` (rounded to 2 decimals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📈 CLV Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:48.746018Z",
     "start_time": "2025-05-30T20:12:48.742240Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_rfm_clv(df):\n",
    "    \"\"\"\n",
    "    Adds RFM scores and 12-month CLV estimate to the input customer-level DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Must include columns ['recency', 'frequency', 'monetary', 'avg_order_value', 'obs_months']\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Original DataFrame with added columns:\n",
    "                      ['recency_score', 'frequency_score', 'monetary_score', 'rfm_total', 'clv_12mo']\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Protect against division by zero\n",
    "    df['obs_months'] = df['obs_months'].clip(lower=1)\n",
    "\n",
    "    # RFM Scores (1 = lowest, 5 = highest)\n",
    "    df['recency_score'] = pd.qcut(-df['recency'], q=5, labels=False, duplicates='drop') + 1\n",
    "    df['frequency_score'] = pd.qcut(df['frequency'], q=5, labels=False, duplicates='drop') + 1\n",
    "    df['monetary_score'] = pd.qcut(df['monetary'], q=5, labels=False, duplicates='drop') + 1\n",
    "\n",
    "    # Total RFM score\n",
    "    df['rfm_total'] = df['recency_score'] + df['frequency_score'] + df['monetary_score']\n",
    "\n",
    "    # Estimate 12-month CLV\n",
    "    df['clv_12mo'] = round(\n",
    "        df['avg_order_value'] * (df['frequency'] / df['obs_months']) * 12, 2\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:49.095308Z",
     "start_time": "2025-05-30T20:12:49.084020Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "cust = compute_rfm_clv(cust)\n",
    "cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:49.477405Z",
     "start_time": "2025-05-30T20:12:49.474934Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "def cap_outliers(df, columns, upper_quantile=0.98):\n",
    "    \"\"\"\n",
    "    Caps values in specified columns at the given upper quantile threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with raw data\n",
    "        columns (list): List of column names to cap\n",
    "        upper_quantile (float): Quantile threshold to cap (default is 0.98)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Modified DataFrame with capped values\n",
    "    \"\"\"\n",
    "    df = df.copy()  # prevent mutation\n",
    "    for col in columns:\n",
    "        upper_limit = df[col].quantile(upper_quantile)\n",
    "        df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:49.765368Z",
     "start_time": "2025-05-30T20:12:49.761689Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "rfm_cols = ['recency', 'frequency', 'monetary']\n",
    "cust = cap_outliers(cust, rfm_cols, upper_quantile=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:50.012547Z",
     "start_time": "2025-05-30T20:12:49.926695Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot 1: Top 10 Customers by CLV\n",
    "# Get top 10 customers by CLV\n",
    "# Fix: Reset index to bring customer_id into columns\n",
    "top_clv = cust.nlargest(10, 'clv_12mo').copy().reset_index()\n",
    "\n",
    "# Then convert customer_id to string for labeling\n",
    "top_clv['customer_id'] = top_clv['customer_id'].astype(int).astype(str)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Add 'hue' and suppress legend to fix Seaborn warning\n",
    "sns.barplot(\n",
    "    x='clv_12mo',\n",
    "    y='customer_id',\n",
    "    hue='customer_id',           # Assign y-axis value as hue for coloring\n",
    "    data=top_clv,\n",
    "    palette='viridis',\n",
    "    legend=False                 # Hide legend since it's redundant\n",
    ")\n",
    "\n",
    "# Styling\n",
    "plt.title('Top 10 Customers by 12-Month CLV')\n",
    "plt.xlabel('Estimated CLV (12 months)')\n",
    "plt.ylabel('Customer ID')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 What the Chart Shows\n",
    "\n",
    "1. The **X-axis** represents estimated **Customer Lifetime Value (CLV)** over the next 12 months — i.e., how much revenue each customer is expected to generate.\n",
    "\n",
    "2. The **Y-axis** shows the **Customer IDs** of your top 10 highest-value customers.\n",
    "\n",
    "3. The **height of each bar** corresponds to the customer’s projected revenue, allowing for easy comparison of value contribution across the top cohort.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💡 Insights from CLV Analysis\n",
    "\n",
    "#### 1. Highly Skewed Value Contribution\n",
    "- A few customers (those around **$35K–$49K**) are significantly more valuable than others.\n",
    "- This suggests a classic **80/20 Pareto pattern** — a small group of customers drives a large share of revenue.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Retention & Upsell Opportunities\n",
    "- The **top 3 customers** have projected values near or above **$36K**.\n",
    "  - These are your **VIPs** and should be prioritized for:\n",
    "    - Loyalty perks\n",
    "    - Upselling opportunities\n",
    "    - Exclusive offers\n",
    "- **Customer ID ≠ Consistency**:\n",
    "  - The spread of customer IDs shows that **high CLV isn't limited to early adopters** — any customer can grow into a top spender.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Strategic Marketing Insight\n",
    "- Targeted retention efforts for the **top 10 customers** can lead to **significant revenue uplift**.\n",
    "- You can build a **lookalike segment** by analyzing behavior patterns of these high-value users:\n",
    "  - Purchase frequency\n",
    "  - Product preferences\n",
    "  - Engagement channels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2: RFM Heatmap (Frequency vs. Recency by Monetary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:50.531916Z",
     "start_time": "2025-05-30T20:12:50.468134Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "rfm_pivot = cust.pivot_table(index='recency_score', columns='frequency_score', values='monetary_score', aggfunc='mean')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(rfm_pivot, annot=True, fmt='.1f', cmap='YlGnBu')\n",
    "plt.title('RFM Heatmap: Avg Monetary Score by Recency & Frequency')\n",
    "plt.xlabel('Frequency Score')\n",
    "plt.ylabel('Recency Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 What the Heatmap Shows\n",
    "\n",
    "1. The **X-axis** represents the **Frequency Score** (1 to 5), where higher scores mean the customer purchases more often.\n",
    "\n",
    "2. The **Y-axis** represents the **Recency Score** (1 to 5), where higher scores mean the customer made a **more recent** purchase.\n",
    "\n",
    "3. Each **cell value** shows the **average Monetary Score** (1 to 5) for customers in that Recency × Frequency combination — i.e., how much they tend to spend.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Insights\n",
    "\n",
    "- **High monetary scores cluster in the top-right corner** (Recency = 5, Frequency = 4 or 5):\n",
    "  These are your **most engaged and high-spending customers** — recent, frequent, and generous.\n",
    "\n",
    "- **Low scores in the bottom-left**:\n",
    "  Customers with **low frequency and old purchases** spend the least — likely churned or low-value.\n",
    "\n",
    "- **Target the 4×4 and 5×4 zones**:\n",
    "  Customers here are **prime candidates** for upselling, loyalty programs, and retention strategies.\n",
    "\n",
    "- **Middle tiers (Recency 3–4, Frequency 2–3)** show stable but moderate spending — you can explore nudging these upward through targeted campaigns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 Boxplots of RFM Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:51.364803Z",
     "start_time": "2025-05-30T20:12:51.249437Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up a 1x3 grid of box plots for RFM features\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "for i, col in enumerate(['recency', 'frequency', 'monetary'], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.boxplot(x=cust[col], color='skyblue')\n",
    "    plt.title(f'Boxplot of {col.capitalize()}')\n",
    "    plt.xlabel(col.capitalize())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart provides a quick visual summary of the **distribution and spread** of the three core RFM metrics across customers:\n",
    "\n",
    "- **Recency** (days since last purchase)\n",
    "- **Frequency** (number of orders)\n",
    "- **Monetary** (total spend)\n",
    "\n",
    "**What to look for:**\n",
    "- The **box** shows the interquartile range (middle 50% of customers).\n",
    "- **Lines** extending from the box show the overall spread (min/max, excluding outliers).\n",
    "- **Dots** are potential outliers—often your top spenders or most frequent buyers.\n",
    "\n",
    "These plots help identify skewed distributions and outliers that may influence clustering or CLV calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Segment Customers by Value & Behavior\n",
    "\n",
    "This section uses a rule-based function to assign customers into meaningful business segments based on their RFM score and predicted 12-month CLV.\n",
    "\n",
    "Defined segments:\n",
    "- **Gold Loyalists**: High RFM + High CLV\n",
    "- **Silver Steady**: Medium RFM + High CLV\n",
    "- **Bronze Bargain-Hunters**: Low RFM\n",
    "- **At-Risk**: Low RFM and CLV\n",
    "\n",
    "This allows us to group customers for targeted retention and acquisition strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:51.691192Z",
     "start_time": "2025-05-30T20:12:51.688272Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "def assign_customer_segment(df, clv_threshold=250):\n",
    "    \"\"\"\n",
    "    Assigns each customer to a segment based on RFM score and CLV value.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Must contain columns ['rfm_total', 'clv_12mo']\n",
    "        clv_threshold (float): Threshold for high CLV customers (default: 250)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Original DataFrame with an added 'segment' column\n",
    "    \"\"\"\n",
    "    def segment_row(row):\n",
    "        if row['rfm_total'] >= 12 and row['clv_12mo'] >= clv_threshold:\n",
    "            return 'Gold Loyalists'\n",
    "        elif 8 <= row['rfm_total'] < 12 and row['clv_12mo'] >= clv_threshold:\n",
    "            return 'Silver Steady'\n",
    "        elif 4 <= row['rfm_total'] < 8:\n",
    "            return 'Bronze Bargain-Hunters'\n",
    "        else:\n",
    "            return 'At-Risk'\n",
    "\n",
    "    df['segment'] = df.apply(segment_row, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌍 Segment Distribution by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:52.074570Z",
     "start_time": "2025-05-30T20:12:52.048777Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Assign segments\n",
    "cust = assign_customer_segment(cust)\n",
    "\n",
    "# Count segments by country\n",
    "segment_summary = cust.groupby(['country', 'segment']).size().reset_index(name='customer_count')\n",
    "\n",
    "# Sort for easier analysis\n",
    "segment_summary = segment_summary.sort_values(['country', 'customer_count'], ascending=[True, False])\n",
    "segment_summary = segment_summary.reset_index(drop=True)\n",
    "\n",
    "segment_summary = segment_summary.sort_values(\n",
    "    by=['country', 'customer_count'],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "gold_loyalists = segment_summary[segment_summary['segment'] == 'Gold Loyalists']\n",
    "gold_loyalists = gold_loyalists.sort_values('customer_count', ascending=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:24:53.990923Z",
     "start_time": "2025-05-30T20:24:53.977246Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "segment_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:24:56.291047Z",
     "start_time": "2025-05-30T20:24:56.286523Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "gold_loyalists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 Interpretation: Segment Distribution by Country\n",
    "\n",
    "The `segment_summary` table reveals how different customer segments are distributed across countries.\n",
    "\n",
    "- The **United Kingdom** has by far the highest concentration of **Gold Loyalists** (354 customers), making it the most valuable market for retention and loyalty efforts.\n",
    "- Other notable countries with high-value customers include **Germany**, **France**, and **Finland**, albeit in much smaller volumes.\n",
    "- Segments like **At-Risk** and **Bronze Bargain-Hunters** are also present in multiple regions, which may indicate churn risk or low spending potential.\n",
    "\n",
    "**Business Insight:**\n",
    "You can use this segmentation to drive **geo-targeted marketing strategies**, focusing loyalty programs and personalized engagement on countries with the highest density of high-CLV customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 Unsupervised Clustering of Customers (RFM Segmentation)\n",
    "\n",
    "This project applied **KMeans clustering** to **RFM data** (Recency, Frequency, Monetary) to uncover customer segments. The workflow included feature engineering, transformation, clustering, dimensionality reduction, and segment interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "**✅ RFM Features Defined:**\n",
    "\n",
    "- **Recency**: Days since the customer's last purchase\n",
    "- **Frequency**: Total number of transactions\n",
    "- **Monetary**: Total revenue generated per customer\n",
    "\n",
    "---\n",
    "\n",
    "**🧪 Data Transformation:**\n",
    "\n",
    "- Features were **standardized** and **log-transformed** to ensure comparability and reduce skewness in distribution.\n",
    "\n",
    "---\n",
    "\n",
    "**🔍 Clustering Approach:**\n",
    "\n",
    "- **KMeans** clustering was used to group customers into distinct segments based on purchasing behavior.\n",
    "- The algorithm works by minimizing within-cluster variance to find natural groupings.\n",
    "\n",
    "---\n",
    "\n",
    "**📉 Dimensionality Reduction:**\n",
    "\n",
    "- **PCA (Principal Component Analysis)** was applied to project the 3D RFM data into 2D space.\n",
    "- This allowed for effective visualization of customer clusters.\n",
    "\n",
    "---\n",
    "\n",
    "**🎯 Segment Interpretation:**\n",
    "\n",
    "- Clusters were labeled into marketing-relevant personas, such as:\n",
    "  - *At-Risk Value*\n",
    "  - *High-Spending Sleepers*\n",
    "  - *Loyal Frequent Buyers*\n",
    "\n",
    "This segmentation can inform targeted marketing strategies and customer retention efforts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:24:36.095964Z",
     "start_time": "2025-05-30T20:24:34.971688Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use only RFM features\n",
    "rfm_features = cust[['recency', 'frequency', 'monetary']].copy()\n",
    "\n",
    "# Apply log(1 + x) transformation to monetary only (already cleaned of negative values)\n",
    "cust['monetary'] = np.log1p(cust['monetary'])\n",
    "\n",
    "# Re-scale the RFM features after log transformation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "rfm_cols = ['recency', 'frequency', 'monetary']\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(cust[rfm_cols])\n",
    "\n",
    "# Use the Elbow Method to determine the optimal number of clusters\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(rfm_scaled)\n",
    "    inertia.append(model.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(rfm_scaled, model.labels_))\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K_range, inertia, marker='o')\n",
    "plt.title('Elbow Method (Inertia)')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(K_range, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Scores')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:55.646439Z",
     "start_time": "2025-05-30T20:12:55.636735Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Apply KMeans with the optimal number of clusters (k=4)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "cust['cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Summarize the characteristics of each cluster\n",
    "cluster_summary = cust.groupby('cluster').agg({\n",
    "    'recency': 'mean',\n",
    "    'frequency': 'mean',\n",
    "    'monetary': 'mean',\n",
    "    'customer_id': 'count'\n",
    "}).rename(columns={'customer_id': 'num_customers'}).reset_index()\n",
    "\n",
    "cluster_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 🔬 PCA & Clustering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:12:55.714370Z",
     "start_time": "2025-05-30T20:12:55.709392Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Run PCA (assuming rfm_scaled is your scaled RFM features)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(rfm_scaled)\n",
    "\n",
    "# Add PCA columns to your main cust DataFrame\n",
    "cust['pca1'] = pca_components[:, 0]\n",
    "cust['pca2'] = pca_components[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:24:17.850670Z",
     "start_time": "2025-05-30T20:24:17.700360Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a function to assign persona names based on cluster IDs\n",
    "def map_personas(df):\n",
    "    persona_map = {\n",
    "        0: 'At-Risk Value',\n",
    "        1: 'Loyal Buyers',\n",
    "        2: 'High-Spending Sleepers',\n",
    "        3: 'New/Casual Buyers'\n",
    "    }\n",
    "    df['persona'] = df['cluster'].map(persona_map)\n",
    "    return df\n",
    "\n",
    "# Apply the persona mapping\n",
    "cust = map_personas(cust)\n",
    "\n",
    "palette = {\n",
    "    'Loyal Buyers': '#1f77b4',         # bold blue\n",
    "    'At-Risk Value': '#d62728',        # strong red\n",
    "    'High-Spending Sleepers': '#2ca02c',  # green\n",
    "    'New/Casual Buyers': '#ff7f0e'     # orange\n",
    "}\n",
    "\n",
    "\n",
    "# Plot the PCA visualization with labeled personas\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    x='pca1', y='pca2',\n",
    "    hue='persona',\n",
    "    palette= palette,\n",
    "    data=cust,\n",
    "    alpha=0.85,\n",
    "    s=80\n",
    ")\n",
    "\n",
    "# Add title and formatting\n",
    "plt.title('Customer Segments by Persona (2D PCA View)', fontsize=14)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Persona', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 Customer Segmentation Analysis (via PCA)\n",
    "\n",
    "The scatterplot above presents a 2D Principal Component Analysis (PCA) projection of customers based on their **Recency**, **Frequency**, and **Monetary (RFM)** scores. Each point represents a customer, and colors correspond to persona segments derived from **KMeans clustering**.\n",
    "\n",
    "#### 🔍 Key Insights:\n",
    "\n",
    "1. **Clear Separation Between Personas**\n",
    "   The PCA projection reveals **distinct clusters** in customer behavior:\n",
    "   - **Loyal Buyers** form a compact group, indicating consistent purchasing activity with high frequency and low recency.\n",
    "   - **High-Spending Sleepers** are spatially distant, suggesting they’ve made large purchases historically but may have lapsed.\n",
    "   - **At-Risk Value** customers cluster separately, representing infrequent or low-spend buyers.\n",
    "\n",
    "2. **Dimensionality Reduction Preserved Segment Structure**\n",
    "   Despite compressing 3 features into 2 dimensions, the plot retains strong visual separation. This validates both the clustering process and the explanatory power of RFM metrics.\n",
    "\n",
    "3. **Actionable Groupings**\n",
    "   The persona labels make it easy to tailor marketing strategies:\n",
    "   - **Loyal Buyers** could be nurtured with loyalty rewards.\n",
    "   - **High-Spending Sleepers** may be re-engaged via personalized win-back campaigns.\n",
    "   - **At-Risk** customers may need deeper incentives or churn-prevention efforts.\n",
    "\n",
    "#### 📊 Technical Notes:\n",
    "- **Log transformation** and **outlier capping** were applied before PCA to reduce skewness and enhance cluster integrity.\n",
    "- **3 clusters** were chosen based on a balance of the **elbow method** and **silhouette scores**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T20:24:23.504950Z",
     "start_time": "2025-05-30T20:24:23.421839Z"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "cluster_summary = (\n",
    "    cust.groupby('persona')\n",
    "    .agg({\n",
    "        'recency': 'mean',\n",
    "        'frequency': 'mean',\n",
    "        'monetary': 'mean',\n",
    "        'customer_id': 'count'  # size of segment\n",
    "    })\n",
    "    .rename(columns={'customer_id': 'num_customers'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# visualize as barplots for each metric\n",
    "melted = cluster_summary.melt(id_vars='persona',\n",
    "                              value_vars=['recency', 'frequency', 'monetary'],\n",
    "                              var_name='Metric', value_name='Value')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=melted, x='Metric', y='Value', hue='persona')\n",
    "plt.title('Average RFM Metrics by Persona')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 Interpreting RFM Averages by Customer Segment\n",
    "\n",
    "This chart visualizes the **average RFM metrics**—Recency, Frequency, and Monetary value—for each of the customer segments discovered through clustering.\n",
    "\n",
    "Each bar represents the **average value** of one metric for one customer persona:\n",
    "\n",
    "#### 🔍 Metrics Explained:\n",
    "- **Recency**: Days since the customer's last purchase (⚠️ lower is better)\n",
    "- **Frequency**: Total number of purchases (✅ higher is better)\n",
    "- **Monetary**: Total revenue generated by the customer (✅ higher is better)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Segment Insights:\n",
    "\n",
    "#### ✅ **Loyal Buyers**\n",
    "- **Low Recency** → These customers purchased recently.\n",
    "- **High Frequency** → They make repeat purchases often.\n",
    "- **High Monetary Value** → They spend a lot annually.\n",
    "- 🔑 **Insight**: This is the company’s most engaged and profitable segment. Marketing teams should prioritize loyalty programs and upsell strategies for them.\n",
    "\n",
    "#### ⚠️ **High-Spending Sleepers**\n",
    "- **Very High Recency** → These customers haven’t purchased in a long time.\n",
    "- **Low Frequency** → They don’t buy often.\n",
    "- **High Monetary Value** → However, when they do buy, they spend a lot.\n",
    "- 🔑 **Insight**: These customers are valuable but dormant. They are ideal targets for win-back or re-engagement campaigns.\n",
    "\n",
    "#### ❌ **At-Risk Value**\n",
    "- **Medium-to-High Recency** → It’s been a while since they purchased.\n",
    "- **Low Frequency** → They have not engaged much.\n",
    "- **Low Monetary Value** → They contribute the least revenue.\n",
    "- 🔑 **Insight**: These customers are low-value and possibly close to churn. Marketing teams could deprioritize them or test discount-based revival campaigns.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Why It Matters:\n",
    "This visualization helps marketing teams:\n",
    "- Personalize campaigns for each persona.\n",
    "- Allocate resources toward high-impact customer groups.\n",
    "- Design retention and reactivation strategies based on behavioral traits.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧠 Insights & Recommendations\n",
    "\n",
    "**Insights**\n",
    "- Gold Loyalists drive the highest revenue despite being a small segment.\n",
    "- At-Risk customers are widespread and show signs of churn.\n",
    "- The Pareto Principle holds — a minority of users generate most value.\n",
    "\n",
    "**Recommendations**\n",
    "- Prioritize loyalty perks for high-CLV segments.\n",
    "- Retarget At-Risk users with personalized offers.\n",
    "- Use top-performer traits to acquire lookalike customers.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
